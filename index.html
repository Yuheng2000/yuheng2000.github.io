<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yuheng Ji</title>

  <meta name="author" content="Yuheng Ji">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ğŸŒ</text></svg>">
      <style>
    .projects-box {
      /* background-color: green; */
      height: 160px;
      position: relative;
      overflow: hidden;
      transition: all ease-in .1s;

    }
    .projects-show {
      padding: 0;
      margin: 0;
      position: absolute;
      bottom: 0;
      left: 0;
      text-align: center;
      height: 30px;
      line-height: 20px;
      width: 100%;
      font-size: 12px;
      color: #0067c8;
      cursor: pointer;
      background: linear-gradient(to bottom, transparent, #fff, #fff);
    }
    .projects-show-text {
      position: relative;
      top: 10px;
    }
  </style>
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yuheng Ji (å†€æ˜±è¡¡)</name>
              </p>

              <p style="text-align:justify">I am a lyric poet, a passionate lover of life, and a PhD candidate at the <a href="http://www.ia.cas.cn/"> Institute of Automation (CASIA)</a>.
                I'm supervised by Prof. <a href="https://people.ucas.edu.cn/~xlzheng">Xiaolong Zheng</a>. My research interests include embodied AI and computer vision.
              </p>

              <p style="text-align:center">
                <a href="jiyuheng2023@ia.ac.cn">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=X4ILYUQAAAAJ&hl=en/">Google Scholar</a> &nbsp/&nbsp
		<a href="data/å°å²›é›†.pdf">Poetry Anthology</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/jiyuheng2.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/jiyuheng3.png" class="hoverZoomLink"></a>
            </td>
          </tr>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My research interests primarily lie in embodied AI and computer vision.
                <br> * denotes equal contributions.
              </p>
            </td>
          </tr>
        </tbody></table>


<!--        paper_list-->
<!--------------------------------------------------------------------------------------------------------------------------->
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>	
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/robobrain2.0.png' width="180" height="120">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://superrobobrain.github.io/">
          <papertitle>RoboBrain 2.0: Technical Report</papertitle>
        </a>
        <br>
        BAAI RoboBrain Team (as core contributor)
        <br>
        <em>arXiv</em>, 2025
        <br>
        <a href="https://superrobobrain.github.io/">Project</a>
        /
        <a href="https://arxiv.org/abs/2507.02029">Paper</a>
        /
        <a href="https://github.com/FlagOpen/RoboBrain2.0">Code</a>
        /
        <a href="https://huggingface.co/collections/BAAI/robobrain20-6841eeb1df55c207a4ea0036">Checkpoints</a>
        <p></p>   
        <p style="text-align:justify">
          We are excited to introduce <strong>RoboBrain2.0</strong>, the most powerful open-source embodied brain model to date. Compared to its predecessor, RoboBrain1.0, our latest version significantly advances multi-agent task planning, spatial reasoning, and closed-loop execution.
</p>
      </td>
</tr>
	<!--------------------------------------------------------------------------------------------------------------------------->
	 <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/robobrain.jpg' width="180" height="110">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://superrobobrain.github.io/">
                  <papertitle>RoboBrain: A Unified Brain Model for Robotic Manipulation from Abstract to Concrete</papertitle>
                </a>
                <br>
	        <strong>Yuheng Ji</strong>*,
		Huajie Tan*,
	        Jiayu Shi*,
		Xiaoshuai Hao*,
		Yuan Zhang,
	        Hengyuan Zhang,
	        Pengwei Wang,
	        Mengdi Zhao,
		Yao Mu,
		Pengju An,
		Xinda Xue,
		Qinghang Su,
		Huaihai Lyu,
		Xiaolong Zheng,
		Jiaming Liu,
		Zhongyuan Wang,
		Shanghang Zhang
                <br>
                <em>CVPR</em>, 2025 (<font color="red"><a href="https://cvpr.thecvf.com/Conferences/2025/News/AI_Enhanced_Robotics" target="_blank" style="color:red; text-decoration: none;">selected in official Embodied AI Trends Commentary</a></font>)
                <br>
    
		            <a href="https://superrobobrain.github.io/robobrainv1">Project</a>
		            /
		            <a href="https://superrobobrain.github.io/robobrainv1">Paper</a>
                /
		            <a href="https://github.com/FlagOpen/RoboBrain">Code</a>
                /
		            <a href="https://huggingface.co/BAAI/RoboBrain">Checkpoints</a>
                /
		            <a href="https://huggingface.co/datasets/BAAI/ShareRobot">Datasets</a>
                <p></p>   
                <p style="text-align:justify">
                We developed <strong>RoboBrain</strong>, an VLM-based model that combines robotic and general multi-modal data, utilizes a multi-stage training strategy, 
			and incorporates long videos and high-resolution images to improve its robotic manipulation capabilities. Extensive experiments demonstrate that RoboBrain achieves SOTA performance across various robotic tasks,
			highlighting its potential to advance robotic brain capabilities.
		</p>
        </td>
	 </tr>




       <!--------------------------------------------------------------------------------------------------------------------------->
	 <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <div class="one">
        <img src='images/reason_rft.png' width="180" height="120">
      </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://tanhuajie.github.io/ReasonRFT/">
        <papertitle>Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning of Vision Language Models</papertitle>
      </a>
      <br>
      Huajie Tan*,
      <strong>Yuheng Ji*</strong>,
      Xiaoshuai Hao*,
      Minglan Lin,
      Pengwei Wang,
      Shanghang Zhang
      <br>
      <em>NeurIPS</em>, 2025
      <br>
      <a href="https://tanhuajie.github.io/ReasonRFT/">Project</a>
      /
      <a href="https://arxiv.org/abs/2503.20752">Paper</a>
      /
      <a href="https://github.com/tanhuajie/Reason-RFT">Code</a>
      /
      <a href="https://github.com/tanhuajie/Reason-RFT?tab=readme-ov-file#--model-zoo">Checkpoints</a>
      /
      <a href="https://huggingface.co/datasets/tanhuajie2001/Reason-RFT-CoT-Dataset">Datasets</a>
      
      <p></p>   
      <p style="text-align:justify">
      We developed <strong>Reason-RFT</strong>, a novel reinforcement fine-tuning framework that enhances visual reasoning capabilities in Vision-Language Models (VLMs). 
Reason-RFT employs a two-phase training strategy: (1) SFT with curated CoT data to activate reasoning potential, followed by 
(2) Group Relative Policy Optimization (GRPO)-based reinforcement learning to generate diverse reasoning-response pairs.
</p>
    </td>
</tr>




       <!--------------------------------------------------------------------------------------------------------------------------->
       <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <img src='images/RoboOS.png' width="180" height="120">
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://github.com/FlagOpen/RoboOS">
            <papertitle>RoboOS: A Unified Memory-based Framework for Lifelong, Scalable, and Robust Multi-Robot Collaboration</papertitle>
          </a>
          <br>
          Huajie Tan*,
          Cheng Chi*,
          Xiansheng Chen*,
          <strong>Yuheng Ji*</strong>,
          Zhongxia Zhao, Xiaoshuai Hao, Yaoxu Lyu, Mingyu Cao, Junkai Zhao, Huaihai Lyu, Enshen Zhou, Ning Chen, Yankai Fu, Cheng Peng, Wei Guo, Dong Liang, Zhuo Chen, Mengsi Lyu, Chenrui He, Yulong Ao, Yonghua Lin, Pengwei Wang, Zhongyuan Wang, Shanghang Zhang
          <br>
          <em>arXiv</em>, 2025
          <br>
          <a href="https://github.com/FlagOpen/RoboOS">Project</a>
          /
          <a href="https://github.com/FlagOpen/RoboOS">Paper</a>
          /
          <a href="https://github.com/FlagOpen/RoboOS">Code</a>

          <p></p>   
          <p style="text-align:justify">
            We present <strong>RoboOS</strong>, a unified memory-based framework for multi-robot collaboration. At its core, the Spatio-Temporalâ€“Embodiment Memory (STEM) integrates spatial, temporal, and embodiment information to support long-horizon learning, heterogeneous coordination, and fault recovery. Experiments in diverse tasks show RoboOS enables lifelong, scalable, and robust collaboration.
    </p>
        </td>
    </tr>


      <!--------------------------------------------------------------------------------------------------------------------------->
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <img src='images/visualtrans.png' width="180" height="120">
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="">
            <papertitle>VisualTrans: A Benchmark for Real-World Visual Transformation Reasoning</papertitle>
          </a>
          <br>
          <strong>Yuheng Ji*</strong>,
          Yipu Wang*,
          Yuyang Liu,
          Xiaoshuai Hao,
          Yue Liu,
          Yuting Zhao,
          Huaihai Lyu,
          Xiaolong Zheng
          <br>
          <em>arXiv</em>, 2025
          <br>
          <a href="http://arxiv.org/abs/2508.04043">Paper</a>
          /
          <a href="https://github.com/WangYipu2002/VisualTrans">Code</a>
          /
          <a href="https://github.com/WangYipu2002/VisualTrans">Datasets</a>
          
          <p></p>
          <p style="text-align:justify">
            VisualTrans is the first real-world benchmark for Visual Transformation Reasoning (VTR), evaluating spatial, procedural and quantitative reasoning across 12 human-object interaction tasks. While current models perform well on static tasks, they show significant limitations in dynamic, multi-step reasoning, revealing critical gaps in temporal and causal understanding for intelligent systems.
    </p>
        </td>
    </tr>
      <!--------------------------------------------------------------------------------------------------------------------------->
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <img src='images/mathsticks.png' width="180" height="180">
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="">
            <papertitle>MathSticks: A Benchmark for Visual Symbolic Compositional Reasoning with Matchstick Puzzles</papertitle>
          </a>
          <br>
          <strong>Yuheng Ji*</strong>,
          Huajie Tan*,
          Cheng Chi*,
          Yijie Xu,
          Yuting Zhao,
          Enshen Zhou,
          Huaihai Lyu,
          Pengwei Wang,
          Zhongyuan, Wang,
          Shanghang Zhang,
          Xiaolong Zheng
          <br>
          <em>NeurIPS MATH-AI Workshop</em>, 2025
          <br>
          <a href="http://arxiv.org/abs/2510.00483">Paper</a>
          /
          <a href="https://github.com/Yuheng2000/MathSticks">Code</a>
          /
          <a href="https://huggingface.co/datasets/yuheng2000/MathSticks">Datasets</a>
          
          <p></p>
          <p style="text-align:justify">
            MathSticks is a benchmark for Visual Symbolic Compositional Reasoning (VSCR) that unifies visual perception, symbolic manipulation, and arithmetic consistency. Each task presents an incorrect matchstick equation in a seven-segment style. The goal is to move exactly one or two sticksâ€”under strict stick-conservation and digit-legibility constraintsâ€”to make the equation mathematically correct.
    </p>
        </td>
    </tr>
   	<!--------------------------------------------------------------------------------------------------------------------------->
     <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/manipulation_survey.png' width="180" height="120">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="">
          <papertitle>Towards a Unified Understanding of Robot Manipulation: A Comprehensive Survey</papertitle>
        </a>
        <br>
        Shuanghao Bai, Wenxuan Song, Jiayi Chen, <strong>Yuheng Ji</strong>, Zhide Zhong, Jin Ynag, Han Zhao, Wanqi Zhou, Wei Zhao, Zhe Li, Pengxiang Ding, Cheng Chi, Haoang Li, Chang Xu, Xiaolong Zheng, Donglin Wang, Shanghang Zhang, Badong Chen
        <br>
        <em>arXiv</em>, 2025
        <br>
        <a href="https://arxiv.org/abs/2510.10903">Paper</a>
        /
        <a href="https://github.com/BaiShuanghao/Awesome-Robotics-Manipulation">Repo</a>
        
        <p></p>
        <p style="text-align:justify">
          Synthesizing over 1200+ publications, this survey fundamentally restructures the landscape of robotic manipulation with a unified taxonomy for planning and control. Critically, we also provide the first systematic dissection of the key bottlenecksâ€”data, utilization, and generalizationâ€”poised to define the next era of progress.
  </p>
      </td>
  </tr>
   <!--------------------------------------------------------------------------------------------------------------------------->
    <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <div class="one">
        <img src='images/ManipLVM.png' width="180" height="100">
      </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://arxiv.org/pdf/2505.16517">
        <papertitle>ManipLVM-R1: Reinforcement Learning for Reasoning in Embodied Manipulation with Large Vision-Language Models</papertitle>
      </a>
      <br>
        Zirui Song*,
        Guangxian Ouyang*,
        Mingzhe Li,
        <strong>Yuheng Ji</strong>,
        Chenxi Wang,
        Zixiang Xu,
        Zeyu Zhang,
        Xiaoqing Zhang,
        Qian Jiang,
        Zhenhao Chen,
        Zhongzhi Li,
        Rui Yan,
        Xiuying Chen
      <br>
      <em>AAAI</em>, 2026
      <br>

      <a href="https://arxiv.org/pdf/2505.16517">Paper</a>

      <p></p>   
      <p style="text-align:justify">
        VLMs enhance robotic manipulation but rely on costly annotated data, limiting OOD adaptability. We propose ManipLVM-R1, a RL framework with Verifiable Rewards (RLVR), replacing supervision to optimize task outcomes for better generalization. Two rule-based rewards drive physical reasoning, achieving strong performance on fewer data (50%) and OOD scenarios.
        </p>
        </td>
        </tr>


	<!--------------------------------------------------------------------------------------------------------------------------->
  <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <div class="one">
        <img src='images/egoprompt.png' width="180"  height="120">
      </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="">
        <papertitle>EgoPrompt: Prompt Learning for Egocentric Action Recognition</papertitle>
      </a>
      <br>
Huaihai Lyu,
Chaofan Chen,
      <strong>Yuheng Ji</strong>,
      Changsheng Xu
      <br>
      <em>ACM MM</em>, 2025
      <br>
      <a href="">Paper</a>
      <p></p>
      <p style="text-align:justify">
        EgoPrompt is a prompt-learning framework for egocentric action recognition that jointly models verbs and nouns by capturing their semantic relationships. It introduces a Unified Prompt Pool and a Diverse Pool Criteria to encourage rich, disentangled representations. EgoPrompt achieves state-of-the-art performance on Ego4D, EPIC-Kitchens, and EGTEA across various generalization benchmarks.
      </p>
</td>
</tr>


	<!--------------------------------------------------------------------------------------------------------------------------->
	 <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/MinGRE.jpg' width="180"  height="120">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2404.13425">
                  <papertitle>Alleviating Performance Disparity in Adversarial Spatiotemporal Graph Learning under Zero-inflated Distribution</papertitle>
                </a>
                <br>
		Songran Bai,
                <strong>Yuheng Ji</strong>,
                Yue Liu,
                Xingwei Zhang,
                Xiaolong Zheng,
		Daniel Dajun Zeng
                <br>
                <em>AAAI</em>, 2025 (<font color="red">Oral</font>)
                <br>
                <a href="https://arxiv.org/pdf/2404.13425">Paper</a>
                <p></p>
                <p style="text-align:justify">
                Spatiotemporal Graph Learning (SGL) under Zero-Inflated Distribution (ZID) is vital for urban risk management but is susceptible to adversarial attacks. 
			Traditional adversarial training (AT) increases performance disparities between classes. 
			We propose the MinGRE framework to reduce these disparities and enhance robustness, promoting more equitable and robust models.
		</p>
      </td>
	 </tr>
	 <!--------------------------------------------------------------------------------------------------------------------------->
	 <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/advlora.jpg' width="180">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2404.13425">
                  <papertitle>Enhancing Adversarial Robustness of Vision-Language Models through Low-Rank Adaptation</papertitle>
                </a>
                <br>
                <strong>Yuheng Ji</strong>*,
                Yue Liu*,
                Zhicheng Zhang,
                Zhao Zhang,
                Yuting Zhao,
		            Xiaoshuai Hao,
                Gang Zhou,
                Xingwei Zhang,
                Xiaolong Zheng
                <br>
                <em>ICMR</em>, 2025
                <br>
                <a href="https://arxiv.org/pdf/2404.13425">Paper</a>
                <p></p>
                <p style="text-align:justify">
                We propose a parameter-efficient adversarial adaptation method named AdvLoRA by low-rank adaptation to improve the robustness of vision-language models.
                </p>
              </td>
	 </tr>
	 <!--------------------------------------------------------------------------------------------------------------------------->
   <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <div class="one">
        <img src='images/FastRSR.jpg' width="180" height="100">
      </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://arxiv.org/pdf/2404.13425">
        <papertitle>FastRSR: Efficient and Accurate Road Surface Reconstruction from Bird's Eye View</papertitle>
      </a>
      <br>
      Yuting Zhao*,
      <strong>Yuheng Ji</strong>*,
      Xiaoshuai Hao,
      Shuxiao Li
      <br>
      <em>ACM MM</em>, 2025
      <br>
      <a href="https://arxiv.org/abs/2504.09535">Paper</a>
      <p></p>
      <p style="text-align:justify">
      Road Surface Reconstruction (RSR) is crucial for autonomous driving, enabling the understanding of road surface conditions.
      Traditional BEV-based methods for transforming perspective views to BEV face challenges such as information loss and representation sparsity.
      We present two innovative BEV-based RSR models: FastRSR-mono and FastRSR-stereo, offering superior efficiency and accuracy, achieving state-of-the-art results in elevation absolute error and processing speed.
</p>
    </td>
</tr>


  	<!--------------------------------------------------------------------------------------------------------------------------->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/robomap.png' width="180" height="100">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://msc-bench.github.io/">
          <papertitle>What Really Matters for Robust Multi-Sensor HD Map Construction?</papertitle>
        </a>
        <br>
        Xiaoshuai Hao,
        Yuting Zhao,
        <strong>Yuheng Ji</strong>,
        Luanyuan Dai,
        Shuai Cheng,
        Rong Yin,
        <br>
        <em>IROS</em>, 2025 (<font color="red">Oral</font>)
        <br>
        <a href="https://arxiv.org/abs/2501.01037">Paper</a>
        <p></p>   
        <p style="text-align:justify">
          This paper enhances HD map construction robustness via data augmentation, a new fusion module, and modality dropout. It improves performance under sensor corruptions and achieves SOTA accuracy on NuScenes.
</p>
      </td>
</tr>



  	<!--------------------------------------------------------------------------------------------------------------------------->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/msc-bench.png' width="180" height="100">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://msc-bench.github.io/">
          <papertitle>MSC-Bench: Benchmarking and Analyzing Multi-Sensor Corruption for Driving Perception</papertitle>
        </a>
        <br>
        Xiaoshuai Hao, Guanqun Liu, Yuting Zhao,
        <strong>Yuheng Ji</strong>,
        Mengchuan Wei, Haimei Zhao, Lingdong Kong, Rong Yin, Yu Liu
        <br>
        <em>ICME</em>, 2025 (<font color="red">Oral</font>)
        <br>
<a href="https://msc-bench.github.io/">Project</a>
/
<a href="https://arxiv.org/abs/2501.01037">Paper</a>
        <p></p>   
        <p style="text-align:justify">
        This work introduces Multi-Sensor Corruption Benchmark (MSC-Bench), 
the first comprehensive benchmark aimed at evaluating the robustness of multi-sensor autonomous driving perception models against various sensor corruptions.
</p>
      </td>
</tr>

	 <!--------------------------------------------------------------------------------------------------------------------------->
   <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <div class="one">
        <img src='images/CCMH.jpg' width="180" height="100">
      </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://link.springer.com/chapter/10.1007/978-981-99-9021-4_48">
        <papertitle>Learning Hash Subspace from Large-Scale Multi-modal Pre-Training: A CLIP-Based Cross-modal Hashing Framework</papertitle>
      </a>
      <br>
      <strong>Yuheng Ji</strong>*,
      Xingwei Zhang*, Gang Zhou, Xiaolong Zheng, Daniel Dajun Zeng
      <br>
      <em>The 11st C2 China</em>, 2023 (<font color="red">Outstanding Paper Award</font>)
      <br>
<a href="https://link.springer.com/chapter/10.1007/978-981-99-9021-4_48">Paper</a>
      <p></p>
      <p style="text-align:justify">
      We propose a cross-modal hashing framework called CCMH (CLIP-based Cross-Modal Hashing), which facilitates the transferability of a well-trained real-value semantic subspace to a hash semantic subspace.
      </p>
    </td>
</tr>
<!--------------------------------------------------------------------------------------------------------------------------->
<!--paper_list-->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Experience</heading>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <ul>
            <li>PhD candidate student @ <a href="http://www.ia.cas.cn/"> Chinese Academy of Science, Institute of Automation (CASIA)</a>, supervised by Prof. <a href="https://people.ucas.edu.cn/~xlzheng">Xiaolong Zheng</a></li>
            <li>Visiting student @ <a href="https://www.baai.ac.cn/"> Beijing Academy of Artificial Intelligence (BAAI)</a> supervised by Prof. <a href="https://scholar.google.com/citations?user=voqw10cAAAAJ&hl=en">Shanghang Zhang</a>, Dr. <a href="https://scholar.google.com/citations?user=2xR6P5AAAAAJ&hl=zh-CN&oi=ao">Pengwei Wang</a> and Dr. <a href="https://scholar.google.com/citations?user=wWGpskcAAAAJ&hl=en&oi=ao">Cheng Chi</a></li>
            <li>Remote visiting student @ <a href="https://www.nus.edu.sg/"> National University of Singapore (NUS)</a>, working with Ph.D. <a href="https://yueliu1999.github.io/">Yue Liu</a></li>
            <!-- <li>Dr. <a href="https://scholar.google.com/citations?user=ui0lvY4AAAAJ&hl=en&oi=ao">Xiaoshuai Hao</a> </li> -->
            <li>Bachelor of engineering @ <a href="http://english.neu.edu.cn/">Northeastern University</a>, supervised by Prof. Miao Fang</li>
            </ul>
        </table>
        

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <ul>
      <li>Reviewer for CVPR'25, ICLR'25, ICMR'25, ICME'25, AAAI'26</li>
            </ul>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Award</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <ul>
            <li>[2025] China National Scholarship for Master's Degree Students, National Award.</li>
	          <li>[2024] Merit Student, UCAS, School Award.</li>
            <li>[2023] Outstanding Graduates, Provincial Award.</li>
            <li>[2022] Recommendation for admission to CASIA.</li>
            <li>[2022] Merit Student, Provincial Award.</li>
            <li>[2022] China National Scholarship for Undergraduate Student, National Award.</li>
            <li>[2021] China National Scholarship for Undergraduate Student, National Award.</li>
            <li>[2020] China National Scholarship for Undergraduate Student, National Award.</li>
            <li>[2019-2023] Scholarships, School Award.</li>
            </ul>
        </table>


	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Others</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <ul>
	    <li>[2024] å†€æ˜±è¡¡, å¼ æ›Œ, éƒ‘æ™“é¾™, "å¤§æ¨¡å‹å¾®è°ƒä¸­çš„ä½ç§©æ€§," ä¸­å›½æŒ‡æŒ¥ä¸æ§åˆ¶å­¦ä¼šé€šè®¯ 55 (1), 44-49.</li>
            <li>[2023] å†€æ˜±è¡¡, å¼ å…´ä¼Ÿ, éƒ‘æ™“é¾™, "åŸºäºå¤šæ¨¡æ€é¢„è®­ç»ƒçš„è·¨æ¨¡æ€æ£€ç´¢ç®—æ³•ç ”ç©¶," ä¸­å›½æŒ‡æŒ¥ä¸æ§åˆ¶å­¦ä¼šé€šè®¯ 46 (4), 10-16.</li>
            <li>[2023] ä¸€ç§åŸºäºå¤šæ¨¡æ€é¢„è®­ç»ƒçš„è·¨æ¨¡æ€å“ˆå¸Œæ£€ç´¢ç³»ç»Ÿï¼Œå‘æ˜ä¸“åˆ©ï¼Œç¬¬ä¸€å‘æ˜äºº</li>
            <li>[2023] ä¸€ç§åŸºäºå›¾ç¥ç»ç½‘ç»œçš„ä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹ç³»ç»Ÿï¼Œå‘æ˜ä¸“åˆ©ï¼Œç¬¬ä¸€å‘æ˜äºº</li>
            <li>[2023] ä¸€ç§é’ˆå¯¹æ£€ç´¢æ¨¡å‹çš„åœ¨çº¿éšç§ä¿æŠ¤ç³»ç»Ÿï¼Œå‘æ˜ä¸“åˆ©ï¼Œç¬¬äºŒå‘æ˜äºº</li>
            <li>[2022] ä¸€ç§åŸºäºæ–°é—»ä¸»é¢˜å¥çš„æ–‡æœ¬æƒ…æ„Ÿåˆ†ç±»ç³»ç»Ÿï¼Œå‘æ˜ä¸“åˆ©ï¼Œç¬¬äºŒå‘æ˜äºº</li>
            </ul>
        </table>

 	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Participation in Research Projects</heading>
	      <p style="margin-bottom: 0.5px;">
                åœ¨æ”»è¯»ç¡•åšæœŸé—´å‚ä¸äº†ä»¥ä¸‹é¡¹ç›®ç ”ç©¶ï¼Œä¸»è¦è´Ÿè´£é¡¹ç›®ä¸­è·¨æ¨¡æ€ä¿¡æ¯è¯­ä¹‰èåˆä¸ç†è§£ç­‰ä¸“é¢˜ç ”ç©¶å·¥ä½œï¼š
              </p>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <ul>
	    <li> åŸºäºå¤šæ¨¡æ€æ•°æ®èåˆçš„æ™ºèƒ½ç¤¾ä¼šé£é™©é¢„è­¦ç ”ç©¶, å›½å®¶è‡ªç„¶ç§‘å­¦åŸºé‡‘é‡ç‚¹é¡¹ç›®.</li>
	    <li> æ–°æŠ€æœ¯é©±åŠ¨çš„å¤æ‚ç¤¾ä¼šç³»ç»Ÿç®¡ç†, å›½å®¶æ°å‡ºé’å¹´ç§‘å­¦åŸºé‡‘é¡¹ç›®.</li>
	    <li> è·¨æ¨¡æ€å¤šè¯­è¨€å¤§æ•°æ®é©±åŠ¨çš„ç¤¾ä¼šé£é™©æ„ŸçŸ¥ä¸ç†è§£, 2030â€”â€œæ–°ä¸€ä»£äººå·¥æ™ºèƒ½â€é‡å¤§é¡¹ç›®.</li>
            </ul>
        </table>
        

        <script>
          let show = false;
          document.querySelector('#projects-show').onclick = function() {
            if (!show) {
              document.querySelector('#projects-box').style.height = 'auto';
              document.querySelector('#projects-box').style.paddingBottom = '20px';
              document.querySelector('#projects-show-text').innerHTML = 'Less';
              show = true;
            } else {
              show = false;
              document.querySelector('#projects-box').style.height = '160px';
              document.querySelector('#projects-box').style.paddingBottom = '0px';
              document.querySelector('#projects-show-text').innerHTML = 'More';
            }
          }
        </script>
</body>
</html>
