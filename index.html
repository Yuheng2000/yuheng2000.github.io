<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yuheng Ji</title>

  <meta name="author" content="Yuheng Ji">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ğŸŒ</text></svg>">
      <style>
    .projects-box {
      /* background-color: green; */
      height: 160px;
      position: relative;
      overflow: hidden;
      transition: all ease-in .1s;

    }
    .projects-show {
      padding: 0;
      margin: 0;
      position: absolute;
      bottom: 0;
      left: 0;
      text-align: center;
      height: 30px;
      line-height: 20px;
      width: 100%;
      font-size: 12px;
      color: #0067c8;
      cursor: pointer;
      background: linear-gradient(to bottom, transparent, #fff, #fff);
    }
    .projects-show-text {
      position: relative;
      top: 10px;
    }
  </style>
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yuheng Ji (å†€æ˜±è¡¡)</name>
              </p>

              <p style="text-align:justify">My name is Yuheng Ji, a lyric poet, a passionate lover of life and a master student at degree Chinese Academy of Science,<a href="http://www.ia.cas.cn/"> Institute of Automation (CASIA)</a>.
                I'm supervised by Prof. <a href="https://people.ucas.edu.cn/~xlzheng">Xiaolong Zheng</a>. My research interests include vision-language models and embodied AI.
              </p>

              <p style="text-align:center">
                <a href="jiyuheng2023@ia.ac.cn">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=X4ILYUQAAAAJ&hl=en/">Google Scholar</a> &nbsp/&nbsp
		<a href="data/å°å²›é›†.pdf">Poetry Anthology</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/jiyuheng2.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/jiyuheng2.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My research interests mainly multi-modal representation learning, efficient modeling and adversarial robust in machine learning.
                <br> * denotes equal contributions.
              </p>
            </td>
          </tr>
        </tbody></table>


<!--        paper_list-->
<!--------------------------------------------------------------------------------------------------------------------------->
 <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	 <!--------------------------------------------------------------------------------------------------------------------------->
	 <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/msc-bench.png' width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://msc-bench.github.io/">
                  <papertitle>MSC-Bench: Benchmarking and Analyzing Multi-Sensor Corruption for Driving Perception</papertitle>
                </a>
                <br>
		Xiaoshuai Hao,
		Guanqun Liu,
		Yuting Zhao,
                <strong>Yuheng Ji</strong>,
                Mengchuan Wei,
                Haimei Zhao,
                Lingdong Kong,
		Rong Yin,
		Yu Liu
                <br>
                <em>arXiv</em>, 2025
                <br>
		<a href="https://msc-bench.github.io/">Project</a>
		/
		<a href="https://arxiv.org/abs/2501.01037">Paper</a>
                <p></p>   
                <p style="text-align:justify">
                This work introduces Multi-Sensor Corruption Benchmark (MSC-Bench), 
		the first comprehensive benchmark aimed at evaluating the robustness of multi-sensor autonomous driving perception models against various sensor corruptions.
		</p>
              </td>
	 </tr>
	<!--------------------------------------------------------------------------------------------------------------------------->
	 <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/MinGRE.jpg' width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2404.13425">
                  <papertitle>Alleviating Performance Disparity in Adversarial Spatiotemporal Graph Learning under Zero-inflated Distribution</papertitle>
                </a>
                <br>
		Songran Bai,
                <strong>Yuheng Ji</strong>,
                Yue Liu,
                Xingwei Zhang,
                Xiaolong Zheng,
		Daniel Dajun Zeng
                <br>
                <em>AAAI (<font color="red">Oral</font>)</em>, 2025
                <br>
                <a href="https://arxiv.org/pdf/2404.13425">Paper</a>
                <p></p>
                <p style="text-align:justify">
                Spatiotemporal Graph Learning (SGL) under Zero-Inflated Distribution (ZID) is vital for urban risk management but is susceptible to adversarial attacks. 
			Traditional adversarial training (AT) increases performance disparities between classes. 
			We propose the MinGRE framework to reduce these disparities and enhance robustness, promoting more equitable and robust models.
		</p>
              </td>
	 </tr>
	 <!--------------------------------------------------------------------------------------------------------------------------->
	 <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/advlora.jpg' width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2404.13425">
                  <papertitle>AdvLoRA: Adversarial Low-Rank Adaptation of Vision-Language Models</papertitle>
                </a>
                <br>
                <strong>Yuheng Ji</strong>*,
                Yue Liu*,
                Zhicheng Zhang,
                Zhao Zhang,
                Yuting Zhao,
                Gang Zhou,
                Xingwei Zhang,
                Xinwang Liu,
                Xiaolong Zheng
                <br>
                <em>Arxiv</em>, 2024
                <br>
                <a href="https://arxiv.org/pdf/2404.13425">Paper</a>
                <p></p>
                <p style="text-align:justify">
                We propose a parameter-efficient adversarial adaptation method named AdvLoRA by low-rank adaptation to improve the robustness of vision-language models.
                </p>
              </td>
	 </tr>
	 <!--------------------------------------------------------------------------------------------------------------------------->
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/CCMH.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://link.springer.com/chapter/10.1007/978-981-99-9021-4_48">
                <papertitle>Learning Hash Subspace from Large-Scale Multi-modal Pre-Training: A CLIP-Based Cross-modal Hashing Framework</papertitle>
              </a>
              <br>
              <strong>Yuheng Ji</strong>*,
              Xingwei Zhang*,
              Gang Zhou,
              Xiaolong Zheng,
              Daniel Dajun Zeng
              <br>
              <em>China Conference on Command and Control (<font color="red">Outstanding Paper Award</font>)</em>, 2023
              <br>
	      <a href="https://link.springer.com/chapter/10.1007/978-981-99-9021-4_48">Paper</a>
              <p></p>
              <p style="text-align:justify">
              We propose a cross-modal hashing framework called CCMH (CLIP-based Cross-Modal Hashing), which facilitates the transferability of a well-trained real-value semantic subspace to a hash semantic subspace.
              </p>
            </td>
	</tr>
<!--------------------------------------------------------------------------------------------------------------------------->
<!--paper_list-->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Experience</heading>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <ul>
	    <li>Remote Visiting Student @ <a href="https://www.nus.edu.sg/"> National University of Singapore (NUS)</a>, working with Ph.D. <a href="https://yueliu1999.github.io/">Yue Liu</a></li>
	    <li>Remote Visiting Student, supervised by Dr. <a href="https://scholar.google.com/citations?user=2xR6P5AAAAAJ&hl=zh-CN&oi=ao">Pengwei Wang</a> and Dr. <a href="https://scholar.google.com/citations?user=ui0lvY4AAAAJ&hl=en&oi=ao">Xiaoshuai Hao</a> </li>
            <li>Master Student @ <a href="http://www.ia.cas.cn/"> Chinese Academy of Science, Institute of Automation (CASIA)</a>, supervised by Prof. <a href="https://people.ucas.edu.cn/~xlzheng">Xiaolong Zheng</a></li>
            <li>Bachelor of Engineering @ <a href="http://english.neu.edu.cn/">Northeastern University</a>, supervised by Prof. Miao Fang</li>
            </ul>
        </table>
        

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <ul>
	    <li>Reviewer for ICME'25</li>
	    <li>Reviewer for CVPR'25</li>
	    <li>Reviewer for ICLR'25</li>
            </ul>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Award</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <ul>
	    <li>[2024] Merit Student, UCAS, School Award.</li>
            <li>[2023] Outstanding Graduates, Provincial Award.</li>
            <li>[2022] Recommendation for admission to CASIA.</li>
            <li>[2022] Merit Student, Provincial Award.</li>
            <li>[2022] China National Scholarship for Undergraduate Student, National Award.</li>
            <li>[2021] China National Scholarship for Undergraduate Student, National Award.</li>
            <li>[2020] China National Scholarship for Undergraduate Student, National Award.</li>
            <li>[2019-2023] Scholarships, School Award.</li>
            </ul>
        </table>


	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Others</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <ul>
	    <li>[2024] å†€æ˜±è¡¡, å¼ æ›Œ, éƒ‘æ™“é¾™, "å¤§æ¨¡å‹å¾®è°ƒä¸­çš„ä½ç§©æ€§," ä¸­å›½æŒ‡æŒ¥ä¸æ§åˆ¶å­¦ä¼šé€šè®¯.</li>
            <li>[2023] å†€æ˜±è¡¡, å¼ å…´ä¼Ÿ, éƒ‘æ™“é¾™, "åŸºäºå¤šæ¨¡æ€é¢„è®­ç»ƒçš„è·¨æ¨¡æ€æ£€ç´¢ç®—æ³•ç ”ç©¶," ä¸­å›½æŒ‡æŒ¥ä¸æ§åˆ¶å­¦ä¼šé€šè®¯ 46 (4), 10-16.</li>
            <li>[2023] ä¸€ç§åŸºäºå¤šæ¨¡æ€é¢„è®­ç»ƒçš„è·¨æ¨¡æ€å“ˆå¸Œæ£€ç´¢ç³»ç»Ÿï¼Œå‘æ˜ä¸“åˆ©ï¼Œç¬¬ä¸€å‘æ˜äºº</li>
            <li>[2023] ä¸€ç§åŸºäºå›¾ç¥ç»ç½‘ç»œçš„ä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹ç³»ç»Ÿï¼Œå‘æ˜ä¸“åˆ©ï¼Œç¬¬ä¸€å‘æ˜äºº</li>
            <li>[2023] ä¸€ç§é’ˆå¯¹æ£€ç´¢æ¨¡å‹çš„åœ¨çº¿éšç§ä¿æŠ¤ç³»ç»Ÿï¼Œå‘æ˜ä¸“åˆ©ï¼Œç¬¬äºŒå‘æ˜äºº</li>
            <li>[2022] ä¸€ç§åŸºäºæ–°é—»ä¸»é¢˜å¥çš„æ–‡æœ¬æƒ…æ„Ÿåˆ†ç±»ç³»ç»Ÿï¼Œå‘æ˜ä¸“åˆ©ï¼Œç¬¬äºŒå‘æ˜äºº</li>
            </ul>
        </table>

 	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Participation in Research Projects</heading>
	      <p style="margin-bottom: 0.5px;">
                åœ¨æ”»è¯»ç¡•åšæœŸé—´å‚ä¸äº†ä»¥ä¸‹é¡¹ç›®ç ”ç©¶ï¼Œä¸»è¦è´Ÿè´£é¡¹ç›®ä¸­è·¨æ¨¡æ€ä¿¡æ¯è¯­ä¹‰èåˆä¸ç†è§£ç­‰ä¸“é¢˜ç ”ç©¶å·¥ä½œï¼š
              </p>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <ul>
	    <li> åŸºäºå¤šæ¨¡æ€æ•°æ®èåˆçš„æ™ºèƒ½ç¤¾ä¼šé£é™©é¢„è­¦ç ”ç©¶, å›½å®¶è‡ªç„¶ç§‘å­¦åŸºé‡‘é‡ç‚¹é¡¹ç›®.</li>
	    <li> æ–°æŠ€æœ¯é©±åŠ¨çš„å¤æ‚ç¤¾ä¼šç³»ç»Ÿç®¡ç†, å›½å®¶æ°å‡ºé’å¹´ç§‘å­¦åŸºé‡‘é¡¹ç›®.</li>
	    <li> ä¿¡æ¯æŠ€æœ¯æ”¯æ’‘å›½å®¶æ²»ç†ç°ä»£åŒ–çš„æˆ˜ç•¥ç ”ç©¶, ä¸­å›½ç§‘å­¦é™¢å­¦éƒ¨é‡å¤§å’¨è¯¢é¡¹ç›®.</li>
	    <li> è·¨æ¨¡æ€å¤šè¯­è¨€å¤§æ•°æ®é©±åŠ¨çš„ç¤¾ä¼šé£é™©æ„ŸçŸ¥ä¸ç†è§£, 2030â€”â€œæ–°ä¸€ä»£äººå·¥æ™ºèƒ½â€é‡å¤§é¡¹ç›®.</li>
            </ul>
        </table>
        

        <p align="center">Design and source code from <a href="https://jonbarron.info/">Jon Barron</a>'s website</p>
        <script>
          let show = false;
          document.querySelector('#projects-show').onclick = function() {
            if (!show) {
              document.querySelector('#projects-box').style.height = 'auto';
              document.querySelector('#projects-box').style.paddingBottom = '20px';
              document.querySelector('#projects-show-text').innerHTML = 'Less';
              show = true;
            } else {
              show = false;
              document.querySelector('#projects-box').style.height = '160px';
              document.querySelector('#projects-box').style.paddingBottom = '0px';
              document.querySelector('#projects-show-text').innerHTML = 'More';
            }
          }
        </script>
</body>
</html>
